# LongNet
The motive of this repo is to try to understand LongNet paper which is a transformer with a billion token context length. We will discuss in detail from scratch on how to calculate the complexity of each of the attention mechanisms and evaluate the algorithms.

Original paper:
https://arxiv.org/pdf/2307.02486.pdf
