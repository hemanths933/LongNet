{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding LongNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Matrix multiplication\n",
    "\n",
    "First we need to understand how matrix mulltiplication work.\n",
    "2d matrix multiplication is N X M can be multiplied with M X P. Meaning both must have common number of dimensions across an axis\n",
    "The result would be N X P dimensions\n",
    "\n",
    "For N- Dimension matrix multiplication, the formula is \n",
    "If both arguments are 2-D they are multiplied like conventional matrices.\n",
    "If either argument is N-D, N > 2, it is treated as a stack of matrices residing in the last two indexes and broadcast accordingly.\n",
    "\n",
    "Ex:\n",
    "\n",
    "Matrix multiplication of 2 matrices with more than 2 dimensions works like this\n",
    "Consider 3X2X5 and 3X5X4 are the 2 matrices\n",
    "so it means in the 1st matrix, there are 3 2X5 2D matrices. Likewise in the 2nd matrix, there are 3 5X4 2D matrices. So multiplying 2D matrices will be (2 X 5) X (5 X 4) = (2 X 4)\n",
    "\n",
    "The 3rd dimension must be same or can be broadcasted. Ex: Now in this case, valid dimensions are (3x2x5, 3x5x4) or (3x2x5, 1x5x4)  or (1x2x5, 3x5x4). Any of these sizes on multiplication gives 3x2x4. If 1 is there, it means it will be broadcasted to other matrix dimension\n",
    "\n",
    "Similarly lets take 5d arrays\n",
    "a = np.ones([6, 2, 4, 7, 4])\n",
    "c = np.ones([6, 1, 1, 4, 3])\n",
    "\n",
    "on multiplication, it gives an array of size (6, 2, 4, 7, 3) because 1 on 2nd dim of c gets broadcasted to 2. 1 on 3rd dimension of c gets broadcasted to 4.\n",
    "\n",
    "So either the other dimensions other than last 2 dimensions should be same or broadcastable and last 2 dimension must follow N X M , M X P -> N X P rule\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Attention Mechanisms\n",
    "\n",
    "Explanation: [TODO]\n",
    "Types: [TODO]\n",
    "How to calculate complexity : [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additive attention:\n",
    "Explanation : [#TODO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # These will store the learnable weights of our attention mechanism\n",
    "        self.query_layer = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key_layer = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.energy_layer = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # Query shape: [batch_size, 1, hidden_dim]\n",
    "        # Key and Value shapes: [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "        query = self.query_layer(query)\n",
    "        key = self.key_layer(key)\n",
    "\n",
    "        # Calculate the attention energies\n",
    "        energies = self.energy_layer(F.tanh(query + key))\n",
    "        print(f\"SCORES is {energies.shape}\")\n",
    "\n",
    "        if mask is not None:\n",
    "            energies = energies.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        # Convert energies to attention probabilities\n",
    "        attention = F.softmax(energies, dim=1)\n",
    "        print(f\"ATTENTION is {attention.shape}\")\n",
    "\n",
    "        # Calculate the context vector\n",
    "        context = torch.bmm(attention.transpose(1, 2), value)\n",
    "        print(f\"OUTPUT is {context.shape}\")\n",
    "        return context, attention\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORES is torch.Size([32, 5, 1])\n",
      "ATTENTION is torch.Size([32, 5, 1])\n",
      "OUTPUT is torch.Size([32, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "attention = AdditiveAttention(hidden_dim=128)\n",
    "out = attention(torch.randn(size=(32,5,128)), torch.randn(size=(32,5,128)), torch.randn(size=(32,5,128)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dot Product Attention:\n",
    "Explanation: [TODO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # Query, key, value shapes: [batch_size, seq_len, d_k]\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        print(f\"SCORES is {scores.shape}\")\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        print(f\"ATTENTION is {attention.shape}\")\n",
    "\n",
    "        # Apply the attention to the values\n",
    "        output = torch.matmul(attention, value)\n",
    "        print(f\"OUTPUT is {output.shape}\")\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORES is torch.Size([32, 5, 5])\n",
      "ATTENTION is torch.Size([32, 5, 5])\n",
      "OUTPUT is torch.Size([32, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "attention = Attention()\n",
    "out = attention(torch.randn(size=(32,5,128)), torch.randn(size=(32,5,128)), torch.randn(size=(32,5,128)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled attention\n",
    "Explanation: [TODO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k):\n",
    "        super().__init__()\n",
    "        self.scaling = 1. / d_k**0.5\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # Query, key, value shapes: [batch_size, n_heads, seq_len, d_k]\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) * self.scaling\n",
    "        print(f\"SCORES is {scores.shape}\")\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        print(f\"ATTENTION is {attention.shape}\")\n",
    "\n",
    "        # Apply the attention to the values\n",
    "        output = torch.matmul(attention, value)\n",
    "        print(f\"OUTPUT is {output.shape}\")\n",
    "        return output, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the module\n",
    "d_k = 64\n",
    "attention_module = ScaledDotProductAttention(d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORES is torch.Size([32, 1, 5, 5])\n",
      "ATTENTION is torch.Size([32, 1, 5, 5])\n",
      "OUTPUT is torch.Size([32, 1, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "out = attention_module(torch.randn(size=(32,1,5,128)), torch.randn(size=(32,1,5,128)), torch.randn(size=(32,1,5,128)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse attention\n",
    "Explanation: [TODO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LongNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
